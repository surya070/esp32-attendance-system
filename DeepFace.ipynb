{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8f2fcb-5913-446b-9773-32391a2e60db",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd7bf61-3c0f-49bc-a3b4-1f8d079dc3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahes\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from deepface import DeepFace\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a707b7-09e2-44b0-a68e-3eaa14a6b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Config\n",
    "DATASET_DIR = \"faces\"          # Folder containing one image per student\n",
    "EMBEDDINGS_FILE = \"embeddings_db.pkl\"\n",
    "MODEL_NAME = \"ArcFace\"           # High accuracy pretrained model\n",
    "INPUT_SIZE = (160, 160)\n",
    "RECOG_THRESHOLD = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95d076-4f49-4dff-ae65-f759b8d5342b",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8e4edf-c022-499b-a6a2-43cbfe832f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    \"\"\"Generate a few augmentations for one face image.\"\"\"\n",
    "    aug_list = [img]\n",
    "    \n",
    "    # Horizontal flip\n",
    "    aug_list.append(cv2.flip(img, 1))\n",
    "    \n",
    "    # Brightness variations\n",
    "    for alpha in [0.8, 1.2]:\n",
    "        bright = cv2.convertScaleAbs(img, alpha=alpha, beta=10)\n",
    "        aug_list.append(bright)\n",
    "    \n",
    "    # Small rotations\n",
    "    h, w = img.shape[:2]\n",
    "    for angle in [-10, 10]:\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1)\n",
    "        rotated = cv2.warpAffine(img, M, (w, h))\n",
    "        aug_list.append(rotated)\n",
    "    \n",
    "    return aug_list\n",
    "\n",
    "def histogram_equalize_color(bgr_img):\n",
    "    \"\"\"Equalize the Y channel of the YCrCb color space.\"\"\"\n",
    "    ycrcb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2YCrCb)\n",
    "    y, cr, cb = cv2.split(ycrcb)\n",
    "    y_eq = cv2.equalizeHist(y)\n",
    "    merged = cv2.merge([y_eq, cr, cb])\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "def align_and_resize(face_img, required_size=INPUT_SIZE):\n",
    "    img_eq = histogram_equalize_color(face_img)\n",
    "    return cv2.resize(img_eq, required_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757a9f1f-5bc7-4225-99aa-e100ff0f0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions for Similarity and Recognition\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    num = np.dot(a, b)\n",
    "    den = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return num / den if den != 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3b945-ac20-4d96-97ed-fdbc7d156bb1",
   "metadata": {},
   "source": [
    "#### Build Embeddings Database (Single Image per Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68fd5458-6216-4d17-8abf-fcddfadeed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for: ['.ipynb_checkpoints', 'Surya']\n",
      "Created augmented embedding for Surya (6 variants)\n",
      "✅ Saved embeddings database: embeddings_db.pkl\n"
     ]
    }
   ],
   "source": [
    "def build_embeddings_db(dataset_path=DATASET_DIR, model_name=MODEL_NAME, out_file=EMBEDDINGS_FILE):\n",
    "    embeddings_db = {}\n",
    "    people = [p for p in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, p))]\n",
    "    print(\"Building embeddings for:\", people)\n",
    "    \n",
    "    for person in people:\n",
    "        folder = os.path.join(dataset_path, person)\n",
    "        img_files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        if len(img_files) == 0:\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(folder, img_files[0])\n",
    "        img = cv2.imread(img_path)\n",
    "        aug_imgs = augment_image(img)\n",
    "        vectors = []\n",
    "        \n",
    "        for aug in aug_imgs:\n",
    "            rep = DeepFace.represent(img_path=aug, model_name=model_name, enforce_detection=False)\n",
    "            if isinstance(rep, list) and len(rep) > 0:\n",
    "                vectors.append(rep[0]['embedding'])\n",
    "        \n",
    "        avg_emb = np.mean(vectors, axis=0)\n",
    "        embeddings_db[person] = avg_emb\n",
    "        print(f\"Created augmented embedding for {person} ({len(aug_imgs)} variants)\")\n",
    "    \n",
    "    with open(out_file, \"wb\") as f:\n",
    "        pickle.dump(embeddings_db, f)\n",
    "    print(\"✅ Saved embeddings database:\", out_file)\n",
    "    return embeddings_db\n",
    "\n",
    "embeddings = build_embeddings_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9202bc00-058d-4cc5-b5bf-b95554d46c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_db(path=EMBEDDINGS_FILE):\n",
    "    with open(path, \"rb\") as f:\n",
    "        db = pickle.load(f)\n",
    "    print(\"Loaded embeddings for:\", list(db.keys()))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2b6b7-a8eb-42c5-affe-f9e1d17eb7b4",
   "metadata": {},
   "source": [
    "#### Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a434d2c1-5059-4a35-96cb-f29c686d410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embedding(face_img, model_name=MODEL_NAME):\n",
    "    try:\n",
    "        rep = DeepFace.represent(img_path=face_img, model_name=model_name, enforce_detection=False)\n",
    "        if isinstance(rep, list) and len(rep) > 0 and 'embedding' in rep[0]:\n",
    "            return np.array(rep[0]['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(\"Embedding error:\", e)\n",
    "    return None\n",
    "\n",
    "def recognize_face(face_img, embeddings_db):\n",
    "    emb = get_face_embedding(face_img)\n",
    "    if emb is None:\n",
    "        return \"Unknown\", 0.0\n",
    "    best_name, best_score = \"Unknown\", -1.0\n",
    "    for name, db_emb in embeddings_db.items():\n",
    "        sim = cosine_similarity(emb, db_emb)\n",
    "        if sim > best_score:\n",
    "            best_score, best_name = sim, name\n",
    "    if best_score < RECOG_THRESHOLD:\n",
    "        best_name = \"Unknown\"\n",
    "    return best_name, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6833d8-40db-40d0-9819-56c4ae782369",
   "metadata": {},
   "source": [
    "#### Real-Time Recognition + Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96770fc-f174-4acb-8cad-d2fe5c21ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(boxA, boxB):\n",
    "    xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])\n",
    "    xB, yB = min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n",
    "    interW, interH = max(0, xB-xA), max(0, yB-yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
    "    boxBArea = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
    "    return interArea / (boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "def run_realtime_recognition(embeddings_db, source=0, detection_interval=10):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open source:\", source)\n",
    "    \n",
    "    trackers = {}\n",
    "    recognition_buffer = {}\n",
    "    next_id = 0\n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"Starting stream... Press 'q' to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        orig = frame.copy()\n",
    "        \n",
    "        # Update existing trackers\n",
    "        for tid, (tracker, label, last_seen) in list(trackers.items()):\n",
    "            ok, bbox = tracker.update(frame)\n",
    "            if not ok:\n",
    "                del trackers[tid]\n",
    "                continue\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "            trackers[tid] = (tracker, label, frame_count)\n",
    "        \n",
    "        # Re-detect every few frames\n",
    "        if frame_count % detection_interval == 0:\n",
    "            gray = cv2.cvtColor(orig, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60,60))\n",
    "            for (x, y, w, h) in faces:\n",
    "                new_box = (x, y, x+w, y+h)\n",
    "                overlap = any(bbox_iou(new_box, (int(t[0].getROI()[0]), int(t[0].getROI()[1]), \n",
    "                                                int(t[0].getROI()[0])+int(t[0].getROI()[2]),\n",
    "                                                int(t[0].getROI()[1])+int(t[0].getROI()[3]))) > 0.3\n",
    "                              for t in trackers.values())\n",
    "                if overlap:\n",
    "                    continue\n",
    "                \n",
    "                face_crop = orig[y:y+h, x:x+w]\n",
    "                if face_crop.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                face_proc = align_and_resize(face_crop)\n",
    "                name, score = recognize_face(face_proc, embeddings_db)\n",
    "                \n",
    "                # Smooth recognition using buffer\n",
    "                recognition_buffer.setdefault(next_id, []).append(name)\n",
    "                if len(recognition_buffer[next_id]) > 5:\n",
    "                    recognition_buffer[next_id].pop(0)\n",
    "                label = Counter(recognition_buffer[next_id]).most_common(1)[0][0]\n",
    "                \n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "                tracker.init(orig, (x, y, w, h))\n",
    "                trackers[next_id] = (tracker, label, frame_count)\n",
    "                next_id += 1\n",
    "        \n",
    "        cv2.imshow(\"Attendance - Recognition\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8cb5484-cd5c-4d5f-8f66-569cf11def89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for: ['.ipynb_checkpoints', 'Surya']\n",
      "Created augmented embedding for Surya (6 variants)\n",
      "✅ Saved embeddings database: embeddings_db.pkl\n"
     ]
    }
   ],
   "source": [
    "embeddings = build_embeddings_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4569425-2b85-4144-a31e-3102a04184e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings for: ['Surya']\n",
      "Starting stream... Press 'q' to quit.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'cv2.TrackerCSRT' object has no attribute 'getROI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_db()\n\u001b[1;32m----> 2\u001b[0m run_realtime_recognition(embeddings, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, detection_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36mrun_realtime_recognition\u001b[1;34m(embeddings_db, source, detection_interval)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     45\u001b[0m     new_box \u001b[38;5;241m=\u001b[39m (x, y, x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh)\n\u001b[1;32m---> 46\u001b[0m     overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(bbox_iou(new_box, (\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m1\u001b[39m]), \n\u001b[0;32m     47\u001b[0m                                     \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     48\u001b[0m                                     \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m3\u001b[39m]))) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m     49\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m trackers\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m overlap:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     45\u001b[0m     new_box \u001b[38;5;241m=\u001b[39m (x, y, x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh)\n\u001b[1;32m---> 46\u001b[0m     overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(bbox_iou(new_box, (\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m1\u001b[39m]), \n\u001b[0;32m     47\u001b[0m                                     \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     48\u001b[0m                                     \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetROI()[\u001b[38;5;241m3\u001b[39m]))) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m     49\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m trackers\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m overlap:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'cv2.TrackerCSRT' object has no attribute 'getROI'"
     ]
    }
   ],
   "source": [
    "embeddings = load_embeddings_db()\n",
    "run_realtime_recognition(embeddings, source=0, detection_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "888ef9f0-5b2a-4dcd-904a-b4f171b86dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings for: ['Surya']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "('Cannot open source:', 'http://192.168.1.50:81/stream')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m esp_stream_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://192.168.1.50:81/stream\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# example\u001b[39;00m\n\u001b[0;32m      2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m load_embeddings_db()\n\u001b[1;32m----> 3\u001b[0m run_realtime_recognition(embeddings, source\u001b[38;5;241m=\u001b[39mesp_stream_url, detection_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mrun_realtime_recognition\u001b[1;34m(embeddings_db, source, detection_interval)\u001b[0m\n\u001b[0;32m     12\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(source)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open source:\u001b[39m\u001b[38;5;124m\"\u001b[39m, source)\n\u001b[0;32m     16\u001b[0m trackers \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     17\u001b[0m recognition_buffer \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ('Cannot open source:', 'http://192.168.1.50:81/stream')"
     ]
    }
   ],
   "source": [
    "esp_stream_url = \"http://192.168.1.50:81/stream\"  # example\n",
    "embeddings = load_embeddings_db()\n",
    "run_realtime_recognition(embeddings, source=esp_stream_url, detection_interval=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f6b7f-f1e2-43ec-9c2f-c287a1201aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
